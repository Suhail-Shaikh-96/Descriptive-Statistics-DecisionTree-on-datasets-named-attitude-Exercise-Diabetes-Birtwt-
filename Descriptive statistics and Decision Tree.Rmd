---
title: "Descriptive Statistics and Decision Tree"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---
# Q.1.a) Summarize the main statistics of all the variables in the data set.
```{r a ,echo=TRUE}
summary(attitude)
```

### 1.b)  How many observations are in the attitude dataset? What function in R did you use to display this information?
```{r b ,echo=TRUE}
nrow(attitude)
dim(attitude)

```

#. 1.c) Produce a scatterplot matrix of the variables in the attitude dataset. What seems to be most correlated with the overall rating?
```{r c ,echo=TRUE}
pairs(attitude,lower.panel =NULL,pch=19,cex=0.5,cex.labels=1.4,cex.axis=1.4)
```

# 1.d) Produce a scatterplot of rating (on the y-axis) vs. learning (on the x-axis). Add a title to the plot using the title() function.
```{r d ,echo=TRUE}
plot(attitude$rating,attitude$learning,pch=19,xlab="Rating",ylab="Learning")
title(main="Learning vs Rating")
```


# 1.e) Produce 2 side-by-side histograms, one for rating and one for learning. You will need to use par(mfrow=...) to get the two plots together.
```{r e ,echo=TRUE}
par(mfrow=c(1,2))

hist(attitude$rating,pch=19,xlab="Rating",main = "Histogram of Rating")
hist(attitude$learning,pch=19,xlab="learning",main = "Histogram of Learning")

```




### 2.a) Make a frequency distribution table for the gender variable to see the frequency distribution
```{r f ,echo=TRUE}
library(readxl)
Exercise <-read_excel("hw1.xls")
View(Exercise)

colnames(Exercise) <- c("weight","height","gender","exerciseperweek","Isregularexercise","friedfoodperweek")
gendertable <- table(Exercise$gender)

```


### 2.b)  Make a bar chart for gender variable
```{r g ,echo=TRUE}
barplot(gendertable,main="Gender Distribution",xlab="Gender",col=c("red","darkblue"))     #col is used for colour
```



### 2.c)  Make a histogram to display the distribution of the Height variable
```{r h ,echo=TRUE}
hist(Exercise$height,main="Height distribution",xlab="Height",col = Exercise$height)
```


### 2.d) Make a cluster bar chart (side-by-side bar chart) to examine the correlation between gender and Ate Fried Food variables.
```{r i ,echo=TRUE}
t <-table(Exercise$gender,Exercise$friedfoodperweek)
barplot(t,beside = TRUE,legend=TRUE,xlab = "No. of times had fried food per week",col = c("red","darkblue"),ylim = c(0,20))
```


### 2.e) Make a scatter plot to examine the correlation between Weight and Height variables, and write a sentence to describe the trend you observed from the scatter plot.
```{r j ,echo=TRUE}
library(ggplot2)
s <-ggplot(Exercise,aes(x=weight,y=height))
s+ geom_point(aes(colour=gender)) +ylim(0,100)+xlim(0,250)
#The graph of height Vs Weight is linear with slope zero.

```


### 2.f) Find the 5-number summary for the Height data and make a boxplot for the Height data with mild and extreme outliers identiﬁed using inner and outer fences. Draw the boxplot.
```{r k ,echo=TRUE}
a <- fivenum(Exercise$height)
#summary(Exercise$height) in summary mean is extra variable which is not needed

IQR <-a[4]-a[2]
boxplot(Exercise$height,main="Boxplot of Height")
IF1 <-a[2]-1.5*IQR
IF2 <- a[4]+1.5*IQR

OF1 <-a[2]-3*IQR
OF2 <-a[4]+3*IQR

f <- ggplot(Exercise,aes(y=height))
f+ geom_boxplot()+ggtitle("Boxplot of Height")+
   geom_hline(yintercept = c(IF1,IF2),linetype="dashed",color="red",size=1.5)+
   geom_hline(yintercept = c(OF1,OF2),linetype="dashed",color="darkblue",size=2)

```



### Q.3.a)All the variables are represented as integer. Write your own function that automatically converts all the integer variables to factors (categorical).
```{r l ,echo=TRUE}
library(MASS)
library(plyr)
library(dplyr)
library(tibble)


#?birthwt
#View(birthwt)
str(birthwt)

converttofactor<-function(x){
  return(as.factor(x))
  
}


str(birthwt)
 #c(1,4:9)
birthwt[,c(1,4,5,6,7,8,9)] <- lapply(birthwt[,c(1,4,5,6,7,8,9)],converttofactor)
str(birthwt)

```


###Q.3.b) Repeat part (a) using mutate() and mapvalues() functions.
```{r m ,echo=TRUE}
library(plyr)
birthwt<-mutate(birthwt, 
         race = as.factor(mapvalues(race , c("white", "black", "other"), c("1","2", "3"))),
         smoke =as.factor(mapvalues(smoke , c("no", "yes"), c("0","1"))),
         ptl = as.factor(mapvalues(ptl , c("no", "yes"), c("0","1"))),
         ht = as.factor(mapvalues(ht , c("no", "yes"), c("0","1"))),
         ui = as.factor(mapvalues(ui , c("no", "yes"), c("0","1"))),
         ftv = as.factor(mapvalues(ftv , c("no", "yes"), c("0","1"))),
         low = as.factor(mapvalues(low , c("no", "yes"), c("0","1"))))

```


### 3.c) Use the tapply() function to see what the average birthweight looks like when broken down by race and smoking status. Does smoking status appear to have an eﬀect on birth weight? Does the eﬀect of smoking status appear to be consistent across racial groups? What is the association between race and birth weight?
```{r n ,echo=TRUE}
tapply(birthwt$bwt, birthwt$race, mean)
tapply(birthwt$bwt, birthwt$smoke, mean)

#Theory

#Yes, it looks like smoking status does have an effect on birth weight as average birth weight for non - smokers is 3055.6, however, for smokers it is 2771.92*
  
 # *Yes, the effect of smoking status appears to be consistent across racial groups as the average birth weight for smokers across all the races is comparatively lesser to non-smokers across the racial groups*
  
  #*We see that Race =1 (white) has the highest average birth weight (3102.72), Race =3 (Other) have a average birth weight of 2805.2 whereas for Race =2 (Black), the average birth weight is the lowest 2719.70*

```


### 3.d) Use kable() function from knitr to dispaly the table you get in part (c)
```{r o ,echo=TRUE}
library(knitr)
#install.packages("kableExtra")
library(kableExtra)
kable(birthwt) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```



### 3.e) Use ddply() function to get the average birthweight by mother’s race and compare it with tapply() function
```{r p ,echo=TRUE}
library(plyr)

table_d<-ddply(birthwt,.(race),summarize,mean_val=mean(bwt))
table_d

```




### 3.f) Use ggplot2() to plot the average birthweight (computed in part (e)) for each race group in a bar plot
```{r q ,echo=TRUE}
library(ggplot2)

ggplot(data=table_d,aes(x=race,y=mean_val),) + geom_bar(stat = "identity",fill='cadetblue4') + xlab("Race") + 
  ylab ("Mean Value") + ggtitle("Average Birth Weight for Each Race Group") + ylim(c(0,4000)) + 
  theme(panel.grid=element_blank(),panel.background = element_blank(),
        axis.line = element_line(colour = "black")) 
```



### 3.g)  Use ddply() function to look at the average birthweight and proportion of babies with low birthweight broken down by smoking status
```{r r ,echo=TRUE}
str(birthwt)

table_g<-ddply(birthwt, ~ smoke, summarize,
      avg_wt = mean(bwt),
      low_birth_prop = mean(low == 1))

table_g

```


### 3.i)  Is the mother’s age correlated with birth weight? Does the correlation vary with smoking status?
```{r s ,echo=TRUE}
ggplot(birthwt,aes(age,bwt,smoke)) + geom_point(aes(color=smoke)) + 
  xlab("Monther's Age") + ylab("Birthweight") + 
  geom_smooth(method='lm',fill='yellow',col='black') +
  ggtitle("Correlation between mother's age and Birth Weight")+
  theme(panel.grid=element_blank(),panel.background = element_blank(),
        axis.line = element_line(colour = "black")) 


#*Yes, the correlation varies with smoking status. We see that most of the data points above the line of best fit are non-smokers, whereas, the data points below the line are smokers. This further shows that smokers during pregnancy have a lower birth rate as compared to the non-smokers*

```




 
 
 ### Q.4.a)  What type of variable is price? Would you expect its distribution to be symmetric, right-skewed, or left-skewed? Why? Make a histogram of the distribution of diamond prices. Does the shape of the distribution match your expectation? (Use geom histogram()).
```{r u ,echo=TRUE}
library(ggplot2)    
#data(diamonds)
View(diamonds)
str(diamonds)

s<- ggplot(diamonds,aes(x=price))
s+ geom_histogram(fill="darkblue",colour="red")+ggtitle("Histogram of Price of diamonds")+ylim(0,15000)

#Price is a dependent variable whose value depend on carat and cut.
#The distribution of variable y is right skew  as frequency of price is goes on decreasing as price increases.


```

 
### Q.4.b) Visualize a few other numerical variables in the dataset and discuss any interesting features. When describing distributions of numerical variables we might also want to view statistics like mean, median, etc.
```{r v ,echo=TRUE}
summary(diamonds)

p<- ggplot(diamonds,aes(x=cut,y=carat))
p+ geom_boxplot(fill="darkblue",colour="red")+ggtitle("Histogram of Price of diamonds")+#points(diamonds$cut, means$carat, col = "red")+
  stat_summary(fun.y=mean, geom="point",colour="red",shape=19,size=4)

```


### Q.4.c) What type of variable is color? Which color is most prominently represented in the dataset?
```{r w ,echo=TRUE}
str(diamonds$color)

#colour is a variable of type factor with 7 levels. COlor G is most prominantly represented in dataset.

```


### Q.4.d)  Make a bar plot of the distribution of cut, and describe its distribution (Use georm bar())
```{r x ,echo=TRUE}
b<- ggplot(diamonds,aes(x=cut))
b+ geom_bar(fill="darkblue",colour="red")+ggtitle("Barplot of cut of diamonds")+ylim(0,25000)
#The graph tells that the frequency of diamonds is increasing as the cut is getting ideal.

```


### Q.4.e)  Make a histogram of the depths of diamonds, with binwidth of 0.2%, and add another variable (say, cut) to the visualization. You can do this either using an aesthetic or a facet. Typical diamonds of which cut have the highest depth? On average, does depth increase or decrease as cut grade increase or decrease?
```{r y ,echo=TRUE}
b<- ggplot(diamonds,aes(x=depth))
b+ geom_histogram(binwidth=0.2,aes(fill=cut),colour="black")+ggtitle("Histogram of depth of diamonds")#+ylim(0,15000)

#By facet_grid

b+ geom_histogram(binwidth=0.2,aes(fill=cut),colour="black")+ggtitle("Histogram of depth of diamonds")+
  facet_grid(cut~.)


#By facet_wrap

b+ geom_histogram(binwidth=0.2,aes(fill=cut),colour="black")+ggtitle("Histogram of depth of diamonds")+
  facet_wrap(cut~.)

#As the cut ibecomes better the depth reduces

```


### Q.4.f) Compare the distribution of price for the diﬀerent cuts. Does anything seem unusual? Describe.
```{r z ,echo=TRUE}
f<- ggplot(diamonds,aes(x=cut,y=price))
f+ geom_boxplot(aes(fill=cut),colour="red")+ggtitle("Boxplot of cut Vs Price")+ylim(0,20000)

#There are many outliers in this boxplot of Price vs Cut. Range for ideal diamond cuts is lesser than other diamond cuts.

```


### Q.4.g) Draw a scatterplot showing the price (y-axis) as a function of the carat (size).
```{r aa ,echo=TRUE}
g<- ggplot(diamonds,aes(x=carat,y=price))
g+ geom_point(aes(fill=cut))+geom_jitter()+ggtitle("Carat Vs Price")+ylim(0,20000)
```


### Q.4.h) Shrink the points in your scatter plot in part (g) using the alpha argument in geom point.
```{r ab ,echo=TRUE}
h<- ggplot(diamonds,aes(x=carat,y=price))
h+ geom_point(alpha=0.05,aes(fill=cut))+ggtitle("Carat Vs Price")+ylim(0,20000)

```


### Q.4.i) Use facet wrap(∼ factor1 + factor2 + ... + factorn) command to create scatter plots showing how diamond price varies with carat size for diﬀerent values of “cut” (use colour = color in aes()).
```{r ac ,echo=TRUE}
i <-ggplot(diamonds,aes(x=carat,y=price))
i+ geom_point(aes(colour=color))+ggtitle("Carat Vs Price")+
  facet_wrap(cut~.,ncol=1)+ylim(0,20000)

```



### Q.5.a) Load the data and check the attributes of the data. How many variables are in this data set?
```{r ad ,echo=TRUE}
setwd("E:\\IDS 572\\Assg1")
library(readxl)
Diabetes <- read_xlsx("Pima Indian Diabetes.xlsx")

#There are 8 independent variable and one dependent variable `Class variable`
```


### Q.5.b) Choose the ﬁrst 80% of the data for training and the remaining 20% data for testing.
```{r ae ,echo=TRUE}
library(e1071)
library(caret)
library(rpart)
library(rpart.plot)

outlier <- function(val){
  iqr <- IQR(val)
  q1 <- as.numeric(quantile(val,.25))
  q3 <- as.numeric(quantile(val,.75))
  upper <- q3+(1.5*iqr)
  
  lower <- q1-(1.5*iqr)
  ifelse ( val < upper & val > lower, val, NA)
  
}
#View(val)

# calling function
diabetes_op<-sapply(Diabetes[,1:8], outlier)

diabetes_cleaned<-data.frame(diabetes_op,Diabetes[,9])
diabetes_F<-na.omit(diabetes_cleaned)
diabetes_F$Class.variable <- as.factor(diabetes_F$Class.variable)

str(diabetes_F)
set.seed(2)
sample=sample(1:nrow(diabetes_F),floor(nrow(diabetes_F)*0.8))
train <-diabetes_F[sample, ]
test <-diabetes_F[-sample,]

```


### Q.5.c)  Use “rpart” function to create a tree using the training data . What is the accuracy of your model based on training data?
```{r af ,echo=TRUE}
#Training the decision tree classifier
tree_model <-rpart(Class.variable~.,data=train,method="class")


#Predictions on training dataset
diabetes.predicted_train<- predict(tree_model,train,type = "class")


#Confusion matrix for evaluating the model on training dataset
confusionMatrix(diabetes.predicted_train,train$Class.variable)

```


### Q.5.d)  Plot your decision tree. How many leaves are in your tree? Are these leaves pure?
```{r ag ,echo=TRUE}
rpart.plot(tree_model)
#There are 15 variables in the tree. All of these leaves are pure.
```


### Q.5.e)  Provide two strongest If-Then rules from this decision tree. Please explain why these rules are chosen.
```{r ah ,echo=TRUE}

#*Two strongest decision rules are ->*

#*1. If Plasma Glucose conc < 144 and Age < 29 Then 0*

#*2. If Plasma Glucose conc >= 144 and Glucose conc >= 155 Then 1*
```


### Q.5.f) What are the most important variables based on your decision tree models?
```{r ai ,echo=TRUE}

# Most important variables are at the top nodes of the tree. Here the most important nodes are ghluccose concentration, age and body mass index.
```


### Q.5.g) Apply the decision tree on test data and report your prediction (just the code is suﬃcient for this part). What is the accuracy of your model on the test data?
```{r aj ,echo=TRUE}
#Predictions on training dataset
diabetes.predicted_test<- predict(tree_model,test,type = "class")

#Confusion matrix for evaluating the model on testing dataset
confusionMatrix(diabetes.predicted_test,test$Class.variable)

```


### Q.5.h) Use a couple of diﬀerent training samples and check how your decision tree models change. Is your decision tree robust?
```{r ak ,echo=TRUE}

#Using  couple of  training datasets we saw the accuracy is similar to the initial accuracy which is 86%. Hence,  the decision tree is robust
```


### Q.5.i) Do parts (c), (e), and (f) for a “ctree” function as well. Are there any signiﬁcant diﬀerences between these decision trees constructed by ctree and rpart?
```{r al ,echo=TRUE}
library(party)
diabetes_ctree<-ctree(Class.variable~.,data = train)
#diabetes_ctree

plot(diabetes_ctree)

#Predictions on training dataset
diabetes.predicted_train_ctree<- predict(diabetes_ctree,train)

#Confusion matrix for evaluating the model on training dataset
confusionMatrix(diabetes.predicted_train_ctree,train$Class.variable)


# Most important variables are at the top nodes of the tree. Here the most important nodes are ghluccose concentration, age and body mass index.

#
```